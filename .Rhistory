library(bit)
library(bslib)
library(cachem)
library(callr)
library(cellranger)
library(cli)
library(clipr)
library(conflicted)
library(tools, lib.loc = "C:/Program Files/R/R-4.5.1/library")
library(translations, lib.loc = "C:/Program Files/R/R-4.5.1/library")
library(tcltk, lib.loc = "C:/Program Files/R/R-4.5.1/library")
library(survival, lib.loc = "C:/Program Files/R/R-4.5.1/library")
library(nnet, lib.loc = "C:/Program Files/R/R-4.5.1/library")
library(nlme, lib.loc = "C:/Program Files/R/R-4.5.1/library")
library(mgcv, lib.loc = "C:/Program Files/R/R-4.5.1/library")
library(Matrix, lib.loc = "C:/Program Files/R/R-4.5.1/library")
library(KernSmooth, lib.loc = "C:/Program Files/R/R-4.5.1/library")
library(grid, lib.loc = "C:/Program Files/R/R-4.5.1/library")
library(foreign, lib.loc = "C:/Program Files/R/R-4.5.1/library")
library(cluster, lib.loc = "C:/Program Files/R/R-4.5.1/library")
library(class, lib.loc = "C:/Program Files/R/R-4.5.1/library")
library(boot, lib.loc = "C:/Program Files/R/R-4.5.1/library")
library(withr)
library(viridisLite)
library(vctrs)
library(uuid)
library(utf8)
library(tidyr)
library(tidyselect)
library(tidyverse)
library(timechange)
library(tinytex)
library(stringi)
library(selectr)
library(readxl)
library(rematch)
library(reprex)
library(rematch2)
detach("package:reprex", unload = TRUE)
library(fs)
library(fontawesome)
library(farver)
---
title: "Exploring Retail Data: A Practical Data Science Project"
**1. Load tidyverse and read the csv file.**
```{r}
```{r}
```{r}
library(tidyverse)
df <- read.csv('customer_data.csv')
names(df)
df[1:5]
df[1:5]```{r}
head(df[1:5])
library(tidyverse)
df <- read.csv('customer_data.csv')
names(df)
head(df[1:5])
# Use lubridate::ymd() to convert 'cc_exp' to Date type and filter for the year 2019
# Try learning new things!!
# use stringr::str_extract() to extract email providers
# Try learning new things!!
# use stringr::str_detect() to find customers with "am.edu" in their email.
# Convert the 'date' to Date type and extract the day of the week
head(df[1:5])
head(df[1:5])
head(df[1:5])
head(df[1:5])
head(df,5)
str(head(df,5))
five_rows = head(df,5)
str(five_rows)
five_rows = head(df,5)
a = str(five_rows)
unique(a)
five_rows = head(df,5)
str(five_rows)
five_rows = head(df,5)
a = str(five_rows)
lapply(a, unique)
five_rows = head(df,5)
str(five_rows)
ncol(df)
nrow(df)
max(df$age)
min(df$age)
mean(df$age)
five_rows = head(df,5)
five_rows
str(five_rows)
full_name =  paste(df$fist,df$last)
name_counts =  table(full_name)
sorted_names <- sort(name_counts, decreasing = TRUE)
head(sorted_names,3)
# Combine first and last names into one full name
full_name <- paste(df$first, df$last)  # Make sure column names are correct
# Count the frequency of each full name
name_counts <- table(full_name)
# Sort names by frequency in descending order
sorted_names <- sort(name_counts, decreasing = TRUE)
# Display the top 3 most common full names
head(sorted_names, 3)
#
# full_name <- paste(df$first, df$last)
#
# name_counts <- table(full_name)
#
# sorted_names <- sort(name_counts, decreasing = TRUE)
#
# head(sorted_names, 3)
library(dplyr)
df |>
mutate(FullName = paste(first, last)) |>      # Create a new column 'FullName'
count(FullName, sort = TRUE) |>               # Count frequency of each full name
slice_head(n = 3)                             # Show top 3
df |>
group_by(phone) |>                  # Group by phone number
filter(n() > 1) |>                  # Keep only those with duplicate phone numbers
arrange(Phone) |>                   # Sort for readability
ungroup()
library(dplyr)
df |>
group_by(phone) |>                  # Group by phone number
filter(n() > 1) |>                  # Keep only those with duplicate phone numbers
arrange(Phone) |>                   # Sort for readability
ungroup()
library(dplyr)
df |>
group_by(phone) |>                  # Group by phone number
filter(n() > 1) |>                  # Keep only those with duplicate phone numbers
arrange(Phone) |>                   # Sort for readability
ungroup()
df |>
filter(!is.na(phone)) |>
group_by(phone) |>
dplyr::filter(n() > 1) |>
arrange(phone) |>
ungroup()
#
# full_name <- paste(df$first, df$last)
#
# name_counts <- table(full_name)
#
# sorted_names <- sort(name_counts, decreasing = TRUE)
#
# head(sorted_names, 3)
df |>
mutate(FullName = paste(first, last)) |>      # Create a new column 'FullName'
count(FullName, sort = TRUE) |>               # Count frequency of each full name
slice_head(n = 3)                             # Show top 3
df |>
filter(!is.na(phone)) |>
# group_by(phone) |>
# dplyr::filter(n() > 1) |>
# arrange(phone) |>
# ungroup()
df |>
filter(!is.na(phone))
library(dbplyr)
library(dtplyr)
library(stats4, lib.loc = "C:/Program Files/R/R-4.5.1/library")
df |>
filter(!is.na(phone))
library(dplyr)
df |>
group_by(phone) |>
dplyr::filter(n() > 1) |>           # Specify dplyr's filter explicitly
arrange(phone) |>
ungroup()
library(dplyr)
df |>
group_by(phone) |>                 # Group rows by 'phone' column
dplyr::filter(n() > 1) |>          # Keep only groups with more than 1 record (duplicate phones)
arrange(phone)                   # Sort the result by 'phone' for easier reading
library(dplyr)
df |>
group_by(phone) |>                 # Group rows by 'phone' column
dplyr::filter(n() > 1) |>          # Keep only groups with more than 1 record (duplicate phones)
arrange(phone) |>                  # Sort the result by 'phone' for easier reading
ungroup()                         # Remove grouping to return a regular dataframe
library(dplyr)
df |>
group_by(phone) |>                 # Group rows by 'phone' column
dplyr::filter(n() > 1) |>          # Keep only groups with more than 1 record (duplicate phones)
arrange(phone) |>                  # Sort the result by 'phone' for easier reading
ungroup()                          # Remove grouping to return a regular dataframe
df |>
filter(profession == "Structural Engineer") |>  # Keep rows where profession is Structural Engineer
nrow()                                          # Count the number of such rows
detach("package:stats", unload = TRUE)
detach("package:stats4", unload = TRUE)
df |>
filter(profession == "Structural Engineer") |>  # Keep rows where profession is Structural Engineer
nrow()                                          # Count the number of such rows
df |>
filter(profession == "Structural Engineer") |>  # Keep rows where profession is Structural Engineer
nrow()                                          # Count the number of such rows
conflicted::conflicts_prefer(dplyr::filter)
df |>
filter(profession == "Structural Engineer") |>  # Keep rows where profession is Structural Engineer
nrow()                                          # Count the number of such rows
df |>
filter(profession == "Structural Engineer", gender == "Male") |>  # Filter rows where profession is Structural Engineer AND gender is Male
nrow()                                                           # Count the number of such rows
df |>
filter(profession == "Structural Engineer",
gender == "Male") |>  # Filter rows where profession is Structural Engineer AND gender is Male
nrow()                                                           # Count the number of such rows
df |>
filter(profession == "Structural Engineer",    # Keep rows with profession Structural Engineer
gender == "Female",                      # Keep only females
province == "AB")                        # Keep only those from Alberta (AB)
df |>
summarise(
max_spending = max(price.CAD., na.rm = TRUE),  # Maximum spending, ignoring missing values
min_spending = min(price.CAD., na.rm = TRUE),  # Minimum spending, ignoring missing values
avg_spending = mean(price.CAD., na.rm = TRUE)  # Average spending, ignoring missing values
)
df |>
filter(price.CAD. == 0) |>
name
df |>
filter(price.CAD. == 0)
df |>
filter(price.CAD. >= 100)
df |>
filter(cc_no == 5020000000000230) |>   # Filter rows where credit card number matches
summarise(email_count = n_distinct(email))   # Count unique email addresses
sum(df$cc_no == 5020000000000230)
# Use lubridate::ymd() to convert 'cc_exp' to Date type and filter for the year 2019
# Try learning new things!!
library(lubridate)  # Load lubridate for date handling
df |>
mutate(exp_date = my(cc_exp)) |>               # Convert "MM/YYYY" to Date (1st of each month assumed)
filter(year(exp_date) == 2019) |>              # Keep only rows where expiration year is 2019
nrow()                                         # Count how many cards expire in 2019
df |>
filter(cc_type == "Visa") |>      # Keep only rows where the credit card type is Visa
nrow()                            # Count how many such rows (people)
df |>
filter(price.CAD. == 100, cc_type == "Visa")   # Find rows where spending is 100 and card type is Visa
df |>
count(profession, sort = TRUE) |>   # Count each profession and sort descending
head(2)                              # Show top 2
# use stringr::str_extract() to extract email providers
# Try learning new things!!
library(stringr)
df |>
mutate(email_provider = str_extract(email, "(?<=@).*")) |>  # Extract part after @ from email
count(email_provider, sort = TRUE) |>                       # Count and sort
head(5)                                                     # Show top 5
# use stringr::str_detect() to find customers with "am.edu" in their email.
df |>
filter(str_detect(email, "am\\.edu"))   # Use regex to find emails containing "am.edu"
# Use lubridate::ymd() to convert 'cc_exp' to Date type and filter for the year 2019
# Try learning new things!!
library(lubridate)  # Load lubridate for date handling
df |>
mutate(exp_date = my(cc_exp))
# Use lubridate::ymd() to convert 'cc_exp' to Date type and filter for the year 2019
# Try learning new things!!
library(lubridate)  # Load lubridate for date handling
df |>
mutate(exp_date = my(cc_exp)) |>               # Convert "MM/YYYY" to Date (1st of each month assumed)
filter(year(exp_date) == 2019)
# Convert the 'date' to Date type and extract the day of the week
library(lubridate)
df |>
mutate(date_parsed = mdy(date)) |>                        # Convert 'date' string to Date object
mutate(day_of_week = wday(date_parsed, label = TRUE)) |>  # Extract weekday name (Mon, Tue...)
count(day_of_week, sort = TRUE)                           # Count customers per weekday
library(lubridate)
# Step 1: Convert cc_exp to Date using my()
df$cc_exp_fixed <- my(df$cc_exp)  # This is fine
# Step 2: Extract year and count how many cards expire in 2019
sum(year(df$cc_exp_fixed) == 2019, na.rm = TRUE)  # FIXED: remove hidden special characters
df |>
filter(cc_type == "Visa") |>      # Keep only rows where the credit card type is Visa
nrow()                            # Count how many such rows (people)
df |>
filter(price.CAD. == 100, cc_type == "Visa")   # Find rows where spending is 100 and card type is Visa
library(tidyverse)
df <- read.csv('customer_data.csv')
names(df)
five_rows = head(df,5)
five_rows
str(five_rows)
ncol(df)
nrow(df)
max(df$age)
min(df$age)
mean(df$age)
#
# full_name <- paste(df$first, df$last)
#
# name_counts <- table(full_name)
#
# sorted_names <- sort(name_counts, decreasing = TRUE)
#
# head(sorted_names, 3)
df |>
mutate(FullName = paste(first, last)) |>      # Create a new column 'FullName'
count(FullName, sort = TRUE) |>               # Count frequency of each full name
slice_head(n = 3)                             # Show top 3
library(dplyr)
df |>
group_by(phone) |>                 # Group rows by 'phone' column
dplyr::filter(n() > 1) |>          # Keep only groups with more than 1 record (duplicate phones)
arrange(phone) |>                  # Sort the result by 'phone' for easier reading
ungroup()                          # Remove grouping to return a regular dataframe
df |>
filter(profession == "Structural Engineer") |>  # Keep rows where profession is Structural Engineer
nrow()                                          # Count the number of such rows
df |>
filter(profession == "Structural Engineer",  gender == "Male") |>  # Filter rows where profession is Structural Engineer AND gender is Male
nrow()                                                           # Count the number of such rows
df |>
filter(profession == "Structural Engineer",    # Keep rows with profession Structural Engineer
gender == "Female",                      # Keep only females
province == "AB")                        # Keep only those from Alberta (AB)
df |>
summarise(
max_spending = max(price.CAD., na.rm = TRUE),  # Maximum spending, ignoring missing values
min_spending = min(price.CAD., na.rm = TRUE),  # Minimum spending, ignoring missing values
avg_spending = mean(price.CAD., na.rm = TRUE)  # Average spending, ignoring missing values
)
df |>
filter(price.CAD. == 0)
df |>
filter(price.CAD. >= 100)
sum(df$cc_no == 5020000000000230)
# Use lubridate::ymd() to convert 'cc_exp' to Date type and filter for the year 2019
# Try learning new things!!
library(lubridate)  # Load lubridate for date handling
df |>
mutate(exp_date = my(cc_exp)) |>               # Convert "MM/YYYY" to Date (1st of each month assumed)
filter(year(exp_date) == 2019) |>              # Keep only rows where expiration year is 2019
nrow()                                         # Count how many cards expire in 2019
df |>
filter(cc_type == "Visa") |>      # Keep only rows where the credit card type is Visa
nrow()                            # Count how many such rows (people)
df |>
filter(price.CAD. == 100, cc_type == "Visa")   # Find rows where spending is 100 and card type is Visa
df |>
count(profession, sort = TRUE) |>   # Count each profession and sort descending
head(2)                              # Show top 2
# use stringr::str_extract() to extract email providers
# Try learning new things!!
library(stringr)
df |>
mutate(email_provider = str_extract(email, "(?<=@).*")) |>  # Extract part after @ from email
count(email_provider, sort = TRUE) |>                       # Count and sort
head(5)                                                     # Show top 5
# use stringr::str_detect() to find customers with "am.edu" in their email.
df |>
filter(str_detect(email, "am\\.edu"))   # Use regex to find emails containing "am.edu"
# Convert the 'date' to Date type and extract the day of the week
library(lubridate)
df |>
mutate(date_parsed = mdy(date)) |>                        # Convert 'date' string to Date object
mutate(day_of_week = wday(date_parsed, label = TRUE)) |>  # Extract weekday name (Mon, Tue...)
count(day_of_week, sort = TRUE)                           # Count customers per weekday
df |>
count(profession, sort = TRUE) |>   # Count each profession and sort descending
head(2)                             # Show top 2
# use stringr::str_extract() to extract email providers
# Try learning new things!!
library(stringr)
df |>
mutate(email_provider = str_extract(email, "(?<=@).*")) |>  # Extract part after @ from email
count(email_provider, sort = TRUE) |>                       # Count and sort
head(5)                                                     # Show top 5
# use stringr::str_detect() to find customers with "am.edu" in their email.
df |>
filter(str_detect(email, "am.edu"))   # Use regex to find emails containing "am.edu"
# Convert the 'date' to Date type and extract the day of the week
library(lubridate)
df |>
mutate(date_parsed = mdy(date)) |>                        # Convert 'date' string to Date object
mutate(day_of_week = wday(date_parsed, label = TRUE)) |>  # Extract weekday name (Mon, Tue...)
count(day_of_week, sort = TRUE)                           # Count customers per weekday
# Convert the 'date' to Date type and extract the day of the week
library(lubridate)
df |>
mutate(date_parsed = mdy(date)) |>                        # Convert 'date' string to Date object
mutate(day_of_week = wday(date_parsed, label = TRUE)) |>  # Extract weekday name (Mon, Tue...)
count(day_of_week, sort = TRUE)                           # Count customers per weekday
# Convert the 'date' to Date type and extract the day of the week
library(lubridate)
df |>
mutate(date_parsed = mdy(date)) |>                        # Convert 'date' string to Date object
mutate(day_of_week = wday(date_parsed, label = TRUE)) |>  # Extract weekday name (Mon, Tue...)
count(day_of_week, sort = TRUE)                           # Count customers per weekday
# Convert the 'date' to Date type and extract the day of the week
library(lubridate)
df |>
mutate(date_parsed = mdy(date)) |>                        # Convert 'date' string to Date object
mutate(day_of_week = wday(date_parsed, label = TRUE)) |>  # Extract weekday name (Mon, Tue...)
count(day_of_week, sort = TRUE)                           # Count customers per weekday
head(1)
# Convert the 'date' to Date type and extract the day of the week
library(lubridate)
df |>
mutate(date_parsed = mdy(date)) |>                        # Convert 'date' string to Date object
mutate(day_of_week = wday(date_parsed, label = TRUE)) |>  # Extract weekday name (Mon, Tue...)
count(day_of_week, sort = TRUE)                           # Count customers per weekday
head(n=1)
# Convert the 'date' to Date type and extract the day of the week
library(lubridate)
df |>
mutate(date_parsed = mdy(date)) |>                        # Convert 'date' string to Date object
mutate(day_of_week = wday(date_parsed, label = TRUE)) |>  # Extract weekday name (Mon, Tue...)
count(day_of_week, sort = TRUE)                           # Count customers per weekday
head(1)
# Convert the 'date' to Date type and extract the day of the week
library(lubridate)
df |>
mutate(date_parsed = mdy(date)) |>                        # Convert 'date' string to Date object
mutate(day_of_week = wday(date_parsed, label = TRUE)) |>  # Extract weekday name (Mon, Tue...)
count(day_of_week, sort = TRUE) |>                        # Count customers per weekday, sorted descending
head(1)                                                   # Show only the top (most frequent) weekday
#
df |>
mutate(FullName = paste(first, last)) |>      # Create a new column 'FullName'
count(FullName, sort = TRUE) |>               # Count frequency of each full name
slice_head(n = 3)                             # Show top 3
five_rows = head(df,5)
five_rows
# str(five_rows)
library(tidyverse)
df <- read.csv('customer_data.csv')
names(df)
five_rows = head(df,5)
five_rows
# str(five_rows)
ncol(df)
nrow(df)
max(df$age)
min(df$age)
mean(df$age)
#
df |>
mutate(FullName = paste(first, last)) |>      # Create a new column 'FullName'
count(FullName, sort = TRUE) |>               # Count frequency of each full name
slice_head(n = 3)                             # Show top 3
library(dplyr)
df |>
group_by(phone) |>                 # Group rows by 'phone' column
dplyr::filter(n() > 1) |>          # Keep only groups with more than 1 record (duplicate phones)
arrange(phone) |>                  # Sort the result by 'phone' for easier reading
ungroup()                          # Remove grouping to return a regular dataframe
df |>
filter(profession == "Structural Engineer") |>  # Keep rows where profession is Structural Engineer
nrow()                                          # Count the number of such rows
df |>
filter(profession == "Structural Engineer",  gender == "Male") |>  # Filter rows where profession is Structural Engineer AND gender is Male
nrow()                                                           # Count the number of such rows
df |>
filter(profession == "Structural Engineer",    # Keep rows with profession Structural Engineer
gender == "Female",                      # Keep only females
province == "AB")                        # Keep only those from Alberta (AB)
df |>
summarise(
max_spending = max(price.CAD., na.rm = TRUE),  # Maximum spending, ignoring missing values
min_spending = min(price.CAD., na.rm = TRUE),  # Minimum spending, ignoring missing values
avg_spending = mean(price.CAD., na.rm = TRUE)  # Average spending, ignoring missing values
)
df |>
filter(price.CAD. == 0)
df |>
filter(price.CAD. >= 100)
sum(df$cc_no == 5020000000000230)
# Use lubridate::ymd() to convert 'cc_exp' to Date type and filter for the year 2019
# Try learning new things!!
library(lubridate)  # Load lubridate for date handling
df |>
mutate(exp_date = my(cc_exp)) |>               # Convert "MM/YYYY" to Date (1st of each month assumed)
filter(year(exp_date) == 2019) |>              # Keep only rows where expiration year is 2019
nrow()                                         # Count how many cards expire in 2019
df |>
filter(cc_type == "Visa") |>      # Keep only rows where the credit card type is Visa
nrow()                            # Count how many such rows (people)
df |>
filter(price.CAD. == 100, cc_type == "Visa")   # Find rows where spending is 100 and card type is Visa
df |>
count(profession, sort = TRUE) |>   # Count each profession and sort descending
head(2)                             # Show top 2
# use stringr::str_extract() to extract email providers
# Try learning new things!!
library(stringr)
df |>
mutate(email_provider = str_extract(email, "(?<=@).*")) |>  # Extract part after @ from email
count(email_provider, sort = TRUE) |>                       # Count and sort
head(5)                                                     # Show top 5
# use stringr::str_detect() to find customers with "am.edu" in their email.
df |>
filter(str_detect(email, "am.edu"))   # Use regex to find emails containing "am.edu"
# Convert the 'date' to Date type and extract the day of the week
library(lubridate)
df |>
mutate(date_parsed = mdy(date)) |>                        # Convert 'date' string to Date object
mutate(day_of_week = wday(date_parsed, label = TRUE)) |>  # Extract weekday name (Mon, Tue...)
count(day_of_week, sort = TRUE) |>                        # Count customers per weekday, sorted descending
head(1)                                                   # Show only the top (most frequent) weekday
five_rows = head(df,5)
five_rows
# str(five_rows)
five_rows = head(df,5)
five_rows
five_rows = head(df,5)
five_rows
str(five_rows)
